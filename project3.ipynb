{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/cse587/spark-2.4.0-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import CountVectorizer, StopWordsRemover, RegexTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import StructField, StringType, StructType, IntegerType, ArrayType, LongType\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator \n",
    "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel, LogisticRegressionWithSGD\n",
    "from pyspark.ml.classification import LogisticRegression, NaiveBayes \n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.feature import NGram\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '6g')\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainD = pd.read_csv(\"train.csv\")\n",
    "# p_schema = [StructField('movie_id',LongType(),True),StructField('movie_name',StringType(),True),StructField('plot',StringType(),True),StructField('genre',ArrayType(),True)]\n",
    "# Fschema = StructType(fields = p_schema)\n",
    "# trainDS = spark.read.schema(p_schema).csv('train.csv')\n",
    "\n",
    "trainDS = sqlContext.createDataFrame(trainD)\n",
    "# trainDS.withColumn(\"genre\", trainDS.genre.cast(ArrayType))\n",
    "\n",
    "testD = pd.read_csv(\"test.csv\")\n",
    "testDS = sqlContext.createDataFrame(testD)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 \n",
    "\n",
    "Tokenize and use Counter Vectorizer here on train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|movie_id|          movie_name|                plot|               words|           Wfiltered|           Nfiltered|            features|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| 1335380|              Exodus|The film is based...|[the, film, is, b...|[film, based, eve...|[film based, base...|(3000,[39,72,80,8...|\n",
      "|29062594|A la salida nos v...|A group of teenag...|[a, group, of, te...|[group, teenagers...|[group teenagers,...|(3000,[76,380],[1...|\n",
      "| 9252321|   Come Back, Africa|This story of a Z...|[this, story, of,...|[story, zulu, fam...|[story zulu, zulu...|(3000,[460,514,70...|\n",
      "|13455076|       A Merry Mixup|The Stooges play ...|[the, stooges, pl...|[stooges, play, t...|[stooges play, pl...|(3000,[8,158,396,...|\n",
      "|24165951|        Getting Even|A soldier-of-fort...|[a, soldier-of-fo...|[soldier-of-fortu...|[soldier-of-fortu...|        (3000,[],[])|\n",
      "| 1925869|  River of No Return|Set in the Northw...|[set, in, the, no...|[set, northwester...|[set northwestern...|(3000,[14,122,216...|\n",
      "|10799612|          Amici miei|Like in many othe...|[like, in, many, ...|[like, many, moni...|[like many, many ...|(3000,[1,3,17,187...|\n",
      "|28238240|Mickey's Big Game...|Mickey and the Sc...|[mickey, and, the...|[mickey, scorpion...|[mickey scorpions...|(3000,[183,1490],...|\n",
      "|17124781|The Good, the Bad...|In the desert wil...|[in, the, desert,...|[desert, wilderne...|[desert wildernes...|(3000,[144,152,16...|\n",
      "|28207941|    The Dancing Fool|Bimbo and Koko ar...|[bimbo, and, koko...|[bimbo, koko, sig...|[bimbo koko, koko...|        (3000,[],[])|\n",
      "|19174305|              Tahaan|Tahaan  lives wit...|[tahaan, , lives,...|[tahaan, , lives,...|[tahaan ,  lives,...|(3000,[55,130,134...|\n",
      "|18392317|     Mysterious Mose|Betty is startled...|[betty, is, start...|[betty, startled,...|[betty startled, ...|(3000,[1409,2809]...|\n",
      "|34420857|Kelviyum Naane Pa...|Nirmal ([[Karthik...|[nirmal, ([[karth...|[nirmal, ([[karth...|[nirmal ([[karthi...|(3000,[28,38,149,...|\n",
      "| 4039635|   First on the Moon|A group of journa...|[a, group, of, jo...|[group, journalis...|[group journalist...|(3000,[418,552,84...|\n",
      "| 8034072|  Journey of a Woman|Vaibhavari Sahay,...|[vaibhavari, saha...|[vaibhavari, saha...|[vaibhavari sahay...|(3000,[1,148,153,...|\n",
      "| 4016437|     Sophie's Choice|In 1947, the movi...|[in, 1947,, the, ...|[1947,, movie's, ...|[1947, movie's, m...|(3000,[174,197,41...|\n",
      "| 1520023|  Ninja Resurrection|Ninja Resurrectio...|[ninja, resurrect...|[ninja, resurrect...|[ninja resurrecti...|(3000,[6,21,24,25...|\n",
      "|24589422|      Mariaâ€™s Lovers|In the spring of ...|[in, the, spring,...|[spring, 1946,, i...|[spring 1946,, 19...|(3000,[5,8,17,18,...|\n",
      "|35068740|           Chinnavar|Muthu ([[Prabhu  ...|[muthu, ([[prabhu...|[muthu, ([[prabhu...|[muthu ([[prabhu,...|(3000,[13,474,475...|\n",
      "|21132951|              Aparan|Vishwanathan , an...|[vishwanathan, ,,...|[vishwanathan, ,,...|[vishwanathan ,, ...|(3000,[1,4,11,44,...|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"plot\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"Wfiltered\", stopWords = stopwords.words(\"english\"))\n",
    "ngram = NGram(n=2, inputCol=\"Wfiltered\", outputCol=\"Nfiltered\")\n",
    "cv_tmp = CountVectorizer(inputCol=\"Nfiltered\", outputCol=\"features\", vocabSize=3000, binary=True, minDF=20.0)\n",
    "\n",
    "pipelineCV = Pipeline(stages=[tokenizer,remover, ngram, cv_tmp])\n",
    "featurizedData = pipelineCV.fit(trainDS)\n",
    "df_vect = featurizedData.transform(trainDS)\n",
    "\n",
    "#########################################################################################################\n",
    "\n",
    "tokenizerTest = Tokenizer(inputCol=\"plot\", outputCol=\"words\")\n",
    "removerTest = StopWordsRemover(inputCol=\"words\", outputCol=\"Wfiltered\", stopWords = stopwords.words(\"english\"))\n",
    "ngramTest = NGram(n=2, inputCol=\"Wfiltered\", outputCol=\"Nfiltered\")\n",
    "cv_tmpTest = CountVectorizer(inputCol=\"Nfiltered\", outputCol=\"features\", vocabSize=3000, binary=True, minDF=10.0)\n",
    "\n",
    "pipelineTest = Pipeline(stages=[tokenizerTest,removerTest, ngramTest, cv_tmpTest])\n",
    "featurizedDataT = pipelineTest.fit(testDS)\n",
    "df_vectT = featurizedDataT.transform(testDS)\n",
    "\n",
    "df_vectT.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the labels in a list to get output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = pd.read_csv('mapping.csv')\n",
    "l1 = {}\n",
    "l1 = [dict(zip(mapper.iloc[i].index.values, mapper.iloc[i].values)) for i in range(len(mapper))]\n",
    "\n",
    "mapperS = sqlContext.createDataFrame(mapper)\n",
    "mvv_array = [row.name for row in mapperS.collect()]\n",
    "\n",
    "def Func(lines):\n",
    "    \n",
    "    lines = lines.strip('][').split(', ')\n",
    "    p = [0] * 20\n",
    "    for i in lines:\n",
    "        for k,v in mapper.iterrows():\n",
    "            if i == ('\\''+v['name']+'\\''):\n",
    "                p[k] = 1\n",
    "                continue\n",
    "    k = str(p).strip('][\\,')\n",
    "    return str(k).replace(\" \", \"\")\n",
    "\n",
    "iplookup_udf = udf(Func)\n",
    "df_vect = df_vect.withColumn(\"op\", iplookup_udf(\"genre\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- op: string (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "newDS=df_vect.select(\"op\",\"features\")\n",
    "newDS.printSchema()\n",
    "# assembler = VectorAssembler(\n",
    "#     inputCols=[\"features\"],\n",
    "#     outputCol=\"features1\")\n",
    "\n",
    "# transformed = assembler.transform(newDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+------+------+------+------+------+------+------+------+------+\n",
      "|                  op|            features|NAME1|NAME2|NAME3|NAME4|NAME5|NAME6|NAME7|NAME8|NAME9|NAME10|NAME11|NAME12|NAME13|NAME14|NAME15|NAME16|NAME17|NAME18|NAME19|NAME20|\n",
      "+--------------------+--------------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+------+------+------+------+------+------+------+------+------+\n",
      "|1,0,0,0,0,1,0,0,0...|  (3000,[608],[1.0])|    1|    0|    0|    0|    0|    1|    0|    0|    0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|\n",
      "|1,0,0,0,1,0,0,0,0...|(3000,[49,234,384...|    1|    0|    0|    0|    1|    0|    0|    0|    0|     0|     1|     0|     0|     0|     0|     0|     0|     1|     0|     0|\n",
      "|1,0,0,0,1,0,0,0,0...|(3000,[1,2,48,72,...|    1|    0|    0|    0|    1|    0|    0|    0|    0|     0|     0|     0|     0|     0|     0|     0|     1|     0|     0|     0|\n",
      "|0,1,0,0,0,0,0,0,0...|(3000,[0,15,42,52...|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|\n",
      "|1,0,0,0,0,1,1,0,0...|(3000,[7,95,440,5...|    1|    0|    0|    0|    0|    1|    1|    0|    0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|\n",
      "+--------------------+--------------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+------+------+------+------+------+------+------+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = newDS\n",
    "\n",
    "split_col = pyspark.sql.functions.split(df['op'], ',')\n",
    "df = df.withColumn('NAME1', split_col.getItem(0))\n",
    "df = df.withColumn('NAME2', split_col.getItem(1))\n",
    "df = df.withColumn('NAME3', split_col.getItem(2))\n",
    "df = df.withColumn('NAME4', split_col.getItem(3))\n",
    "df = df.withColumn('NAME5', split_col.getItem(4))\n",
    "df = df.withColumn('NAME6', split_col.getItem(5))\n",
    "df = df.withColumn('NAME7', split_col.getItem(6))\n",
    "df = df.withColumn('NAME8', split_col.getItem(7))\n",
    "df = df.withColumn('NAME9', split_col.getItem(8))\n",
    "df = df.withColumn('NAME10', split_col.getItem(9))\n",
    "df = df.withColumn('NAME11', split_col.getItem(10))\n",
    "df = df.withColumn('NAME12', split_col.getItem(11))\n",
    "df = df.withColumn('NAME13', split_col.getItem(12))\n",
    "df = df.withColumn('NAME14', split_col.getItem(13))\n",
    "df = df.withColumn('NAME15', split_col.getItem(14))\n",
    "df = df.withColumn('NAME16', split_col.getItem(15))\n",
    "df = df.withColumn('NAME17', split_col.getItem(16))\n",
    "df = df.withColumn('NAME18', split_col.getItem(17))\n",
    "df = df.withColumn('NAME19', split_col.getItem(18))\n",
    "df = df.withColumn('NAME20', split_col.getItem(19))\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "\n",
    "listop = []\n",
    "listl = ['Drama','Comedy','Romance Film','Thriller','Action','World cinema','Crime Fiction','Horror','Black-and-white',\n",
    "         'Indie','Action/Adventure','Adventure','Family Film','Short Film','Romantic drama','Animation','Musical','Science Fiction',\n",
    "        'Mystery','Romantic comedy']\n",
    "\n",
    "\n",
    "ops = pd.DataFrame()\n",
    "ops['movie_id'] = [int(row.movie_id) for row in df_vectT.collect()]\n",
    "\n",
    "for i in range(len(listl)):\n",
    "    dramadf = df.select('features','NAME'+str(i+1))\n",
    "    dramadf = dramadf.withColumnRenamed('NAME'+str(i+1),'label')\n",
    "    dramadf = dramadf.withColumn(\"label\", dramadf[\"label\"].cast(IntegerType()))\n",
    "    \n",
    "    ################ set pipeline ########################\n",
    "    lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=8)\n",
    "    modelN = lr.fit(dramadf)\n",
    "    modelN.save(os.path.join(sys.argv[1], 'model_'+str(i+1)))\n",
    "    predictionsN = modelN.transform(df_vectT)\n",
    "    \n",
    "    ################ store ouputs offline ##################\n",
    "    mvv_array = [int(row.prediction) for row in predictionsN.collect()]\n",
    "    ops['op'+str(i+1)] = mvv_array\n",
    "\n",
    "cols = ['op1','op2','op3','op4','op5','op6','op7','op8','op9','op10',\n",
    "        'op11','op12','op13','op14','op15','op16','op17','op18','op19','op20']\n",
    "\n",
    "\n",
    "ops.head()\n",
    "FinalOutput = pd.DataFrame() \n",
    "FinalOutput['movie_id'] = ops['movie_id']\n",
    "FinalOutput['predictions'] = ops[cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "FinalOutput.to_csv('OutputDemo.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check F1 score for the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1DF = df.select('features','NAME2')\n",
    "F1DF = F1DF.withColumnRenamed('NAME2','label')\n",
    "F1DF = F1DF.withColumn(\"label\", F1DF[\"label\"].cast(IntegerType()))\n",
    "training, testing = F1DF.randomSplit([0.7,0.3], seed=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
    "lrModel = lr.fit(training)\n",
    "predictions = lrModel.transform(testing)\n",
    "\n",
    "# (c.select(col(\"label\").alias(\"label\"), col(\"features1\"))\n",
    "#   .rdd\n",
    "#   .map(lambda row: LabeledPoint(row.label, row.features)))\n",
    "# temp = newDS.map(lambda line:LabeledPoint(line[7],[line[6]]))\n",
    "# temp.take(5)\n",
    "\n",
    "# from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "# predictions = model.transform(trainingData)\n",
    "# evaluator = ClusteringEvaluator()\n",
    "# centers = model.clusterCenters()\n",
    "# silhouette = evaluator.evaluate(predictions)\n",
    "# print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "# model = RandomForestClassifier(featuresCol = 'features', labelCol='label')\n",
    "# rf_model = model.fit(newDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       accuracy:0.6927875547085338:\n"
     ]
    }
   ],
   "source": [
    "evaluator_cv_lr = MulticlassClassificationEvaluator().setPredictionCol(\"prediction\").evaluate(predictions)\n",
    "print('                       accuracy:{}:'.format(evaluator_cv_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 \n",
    "\n",
    "Applying the TF-IDF for vectorizing the inputs better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"plot\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"Wfiltered\")\n",
    "# ngram = NGram(n=2, inputCol=\"Wfiltered\", outputCol=\"Nfiltered\")\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"Wfiltered\", outputCol=\"hashed_features\", numFeatures=10000)\n",
    "idf_stage = IDF(inputCol=\"hashed_features\", outputCol=\"features\", minDocFreq=4)\n",
    "\n",
    "pipelineCV = Pipeline(stages=[tokenizer,remover,hashingTF, idf_stage])\n",
    "featurizedtrainTF = pipelineCV.fit(trainDS)\n",
    "df_trianTDF = featurizedtrainTF.transform(trainDS)\n",
    "\n",
    "\n",
    "featurizedtestTF = pipelineCV.fit(trainDS)\n",
    "df_testTDF = featurizedtestTF.transform(testDS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vectTDF = df_trianTDF.withColumn(\"op\", iplookup_udf(\"genre\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.classification import LinearSVC\n",
    "# F1DFTF = dfTF.select('features','NAME2')\n",
    "# F1DFTF = F1DFTF.withColumnRenamed('NAME2','label')\n",
    "# F1DFTF = F1DFTF.withColumn(\"label\", F1DFTF[\"label\"].cast(IntegerType()))\n",
    "# trainingTF, testingTF = F1DFTF.randomSplit([0.7,0.3], seed=100)\n",
    "\n",
    "# lrTF = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
    "# lrModelTF = lrTF.fit(trainingTF)\n",
    "# predictionsTF = lrModelTF.transform(testingTF)\n",
    "\n",
    "# evaluator_cv_lrTF = MulticlassClassificationEvaluator().setPredictionCol(\"prediction\").evaluate(predictionsTF)\n",
    "# print('                       accuracy:{}:'.format(evaluator_cv_lrTF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+------+------+------+------+------+------+------+------+------+\n",
      "|            features|                  op|NAME1|NAME2|NAME3|NAME4|NAME5|NAME6|NAME7|NAME8|NAME9|NAME10|NAME11|NAME12|NAME13|NAME14|NAME15|NAME16|NAME17|NAME18|NAME19|NAME20|\n",
      "+--------------------+--------------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+------+------+------+------+------+------+------+------+------+\n",
      "|(10000,[135,265,7...|1,0,0,0,0,1,0,0,0...|    1|    0|    0|    0|    0|    1|    0|    0|    0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|\n",
      "|(10000,[26,47,66,...|1,0,0,0,1,0,0,0,0...|    1|    0|    0|    0|    1|    0|    0|    0|    0|     0|     1|     0|     0|     0|     0|     0|     0|     1|     0|     0|\n",
      "|(10000,[52,74,104...|1,0,0,0,1,0,0,0,0...|    1|    0|    0|    0|    1|    0|    0|    0|    0|     0|     0|     0|     0|     0|     0|     0|     1|     0|     0|     0|\n",
      "|(10000,[52,76,77,...|0,1,0,0,0,0,0,0,0...|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|\n",
      "|(10000,[15,137,21...|1,0,0,0,0,1,1,0,0...|    1|    0|    0|    0|    0|    1|    1|    0|    0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|\n",
      "|(10000,[13,52,158...|1,0,0,1,1,0,0,0,0...|    1|    0|    0|    1|    1|    0|    0|    0|    0|     0|     1|     0|     0|     0|     0|     0|     0|     0|     0|     0|\n",
      "|(10000,[15,91,104...|1,0,0,1,0,0,0,1,0...|    1|    0|    0|    1|    0|    0|    0|    1|    0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|\n",
      "|(10000,[17,24,77,...|1,0,0,0,0,0,0,0,0...|    1|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|\n",
      "|(10000,[1,35,78,1...|1,1,1,0,0,0,0,0,1...|    1|    1|    1|    0|    0|    0|    0|    0|    1|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     1|\n",
      "|(10000,[327,547,5...|0,0,0,0,0,0,0,0,0...|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|     0|     1|     1|     0|     1|     0|     0|     0|     0|\n",
      "|(10000,[49,100,11...|0,1,0,0,0,0,0,0,0...|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|\n",
      "|(10000,[76,468,99...|1,1,0,0,0,1,1,0,0...|    1|    1|    0|    0|    0|    1|    1|    0|    0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|\n",
      "|(10000,[410,614,7...|0,1,0,0,0,0,0,0,0...|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|\n",
      "|(10000,[15,52,104...|0,1,0,0,0,0,0,0,0...|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|\n",
      "|(10000,[61,87,109...|0,0,0,0,0,0,0,1,0...|    0|    0|    0|    0|    0|    0|    0|    1|    0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|\n",
      "|(10000,[213,493,7...|0,0,0,1,0,0,1,1,0...|    0|    0|    0|    1|    0|    0|    1|    1|    0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     1|     0|\n",
      "|(10000,[433,436,5...|1,0,0,0,0,0,0,0,0...|    1|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|\n",
      "|(10000,[164,347,3...|0,0,1,1,1,0,1,0,0...|    0|    0|    1|    1|    1|    0|    1|    0|    0|     1|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|\n",
      "|(10000,[306,367,4...|1,0,0,0,0,0,0,0,0...|    1|    0|    0|    0|    0|    0|    0|    0|    0|     1|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|\n",
      "|(10000,[87,207,21...|1,0,0,0,0,0,0,0,0...|    1|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|\n",
      "+--------------------+--------------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+------+------+------+------+------+------+------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vectTDF = df_vectTDF.withColumn(\"op\", iplookup_udf(\"genre\"))\n",
    "NewdfTF = df_vectTDF.select('features','op')\n",
    "split_col = pyspark.sql.functions.split(NewdfTF['op'], ',')\n",
    "for i in range(20):\n",
    "    NewdfTF = NewdfTF.withColumn('NAME'+str(i+1), split_col.getItem(i))\n",
    "\n",
    "NewdfTF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################## TF IDF #################################################\n",
    "path = 'model_rfcTF'\n",
    "os.mkdir(path)\n",
    "\n",
    "listop = []\n",
    "listl = ['Drama','Comedy','Romance Film','Thriller','Action','World cinema','Crime Fiction','Horror','Black-and-white',\n",
    "         'Indie','Action/Adventure','Adventure','Family Film','Short Film','Romantic drama','Animation','Musical','Science Fiction',\n",
    "        'Mystery','Romantic comedy']\n",
    "opsTF = pd.DataFrame()\n",
    "opsTF['movie_id'] = [int(row.movie_id) for row in df_testTDF.collect()]\n",
    "\n",
    "for i in range(len(listl)):\n",
    "    dramadf = NewdfTF.select('features','NAME'+str(i+1))\n",
    "    dramadf = dramadf.withColumnRenamed('NAME'+str(i+1),'label')\n",
    "    dramadf = dramadf.withColumn(\"label\", dramadf[\"label\"].cast(IntegerType()))\n",
    "    \n",
    "    ################ set pipeline ########################\n",
    "    lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=8)\n",
    "    modelN = lr.fit(dramadf)\n",
    "    modelN.save(os.path.join(sys.argv[1], 'model_'+str(i+1)))\n",
    "    predictionsN = modelN.transform(df_testTDF)\n",
    "    \n",
    "    ################ store ouputs offline ##################\n",
    "    mvv_array = [int(row.prediction) for row in predictionsN.collect()]\n",
    "    opsTF['op'+str(i+1)] = mvv_array\n",
    "\n",
    "colsTF = ['op1','op2','op3','op4','op5','op6','op7','op8','op9','op10',\n",
    "        'op11','op12','op13','op14','op15','op16','op17','op18','op19','op20']\n",
    "\n",
    "opsTF.head()\n",
    "FinalOutputTF = pd.DataFrame() \n",
    "FinalOutputTF['movie_id'] = opsTF['movie_id']\n",
    "FinalOutputTF['predictions'] = opsTF[colsTF].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "FinalOutputTF.to_csv('OutputTFNew.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Word to Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|movie_id|        movie_name|                plot|               genre|               words|           Wfiltered|             Lfilter|            features|\n",
      "+--------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|23890098|        Taxi Blues|Shlykov, a hard-w...|['World cinema', ...|[shlykov, a, hard...|[shlykov, hard, w...|[shlykov, hard, w...|[0.03008223511963...|\n",
      "|31186339|  The Hunger Games|The nation of Pan...|['Action/Adventur...|[the, nation, of,...|[nation, panem, c...|[nation, panem, c...|[0.04306361994775...|\n",
      "|20663735|        Narasimham|Poovalli Induchoo...|['Musical', 'Acti...|[poovalli, induch...|[poovalli, induch...|[pooval, induchoo...|[-0.0210946972665...|\n",
      "| 2231378|The Lemon Drop Kid|The Lemon Drop Ki...|          ['Comedy']|[the, lemon, drop...|[lemon, drop, kid...|[lemon, drop, kid...|[0.02432931015478...|\n",
      "|  595909| A Cry in the Dark|Seventh-day Adven...|['Crime Fiction',...|[seventh, day, ad...|[seventh, day, ad...|[seventh, day, ad...|[0.01604021873983...|\n",
      "+--------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "tokenizer = RegexTokenizer(inputCol=\"plot\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "tokenDF = tokenizer.transform(trainDS)\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"Wfiltered\")\n",
    "removeDF = remover.transform(tokenDF)\n",
    "\n",
    "stem = SnowballStemmer(language='english')\n",
    "stemmer_udf = udf(lambda tks: [stem.stem(row) for row in tks], ArrayType(StringType()))\n",
    "stemmed = removeDF.withColumn(\"Lfilter\", stemmer_udf(\"Wfiltered\"))\n",
    "\n",
    "word2Vec = Word2Vec(vectorSize=250, seed=43, inputCol=\"Lfilter\", outputCol=\"features\", minCount=5)\n",
    "\n",
    "pipelineW2V = Pipeline(stages=[word2Vec])\n",
    "featurizedtrainW2V = pipelineW2V.fit(stemmed)\n",
    "df_trianW2V = featurizedtrainW2V.transform(stemmed)\n",
    "\n",
    "\n",
    "tokenDFTest = tokenizer.transform(testDS)\n",
    "removeDFTest = remover.transform(tokenDFTest)\n",
    "stemmed = removeDFTest.withColumn(\"Lfilter\", stemmer_udf(\"Wfiltered\"))\n",
    "\n",
    "featurizedtestW2V = pipelineW2V.fit(stemmed)\n",
    "df_testW2V = featurizedtestW2V.transform(stemmed)\n",
    "\n",
    "df_trianW2V.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vectW2V = df_trianW2V.withColumn(\"op\", iplookup_udf(\"genre\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|movie_id|          movie_name|                plot|               genre|               words|           Wfiltered|             Lfilter|            features|                  op|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|23890098|          Taxi Blues|Shlykov, a hard-w...|['World cinema', ...|[shlykov, a, hard...|[shlykov, hard, w...|[shlykov, hard, w...|[0.03008223511963...|1,0,0,0,0,1,0,0,0...|\n",
      "|31186339|    The Hunger Games|The nation of Pan...|['Action/Adventur...|[the, nation, of,...|[nation, panem, c...|[nation, panem, c...|[0.04306361994775...|1,0,0,0,1,0,0,0,0...|\n",
      "|20663735|          Narasimham|Poovalli Induchoo...|['Musical', 'Acti...|[poovalli, induch...|[poovalli, induch...|[pooval, induchoo...|[-0.0210946972665...|1,0,0,0,1,0,0,0,0...|\n",
      "| 2231378|  The Lemon Drop Kid|The Lemon Drop Ki...|          ['Comedy']|[the, lemon, drop...|[lemon, drop, kid...|[lemon, drop, kid...|[0.02432931015478...|0,1,0,0,0,0,0,0,0...|\n",
      "|  595909|   A Cry in the Dark|Seventh-day Adven...|['Crime Fiction',...|[seventh, day, ad...|[seventh, day, ad...|[seventh, day, ad...|[0.01604021873983...|1,0,0,0,0,1,1,0,0...|\n",
      "| 5272176|            End Game|The president is ...|['Action/Adventur...|[the, president, ...|[president, way, ...|[presid, way, giv...|[0.06528372014330...|1,0,0,1,1,0,0,0,0...|\n",
      "| 1952976|          Dark Water|{{plot}} The film...|['Thriller', 'Dra...|[plot, the, film,...|[plot, film, open...|[plot, film, open...|[0.03373993718494...|1,0,0,1,0,0,0,1,0...|\n",
      "|24225279|                Sing|The story begins ...|           ['Drama']|[the, story, begi...|[story, begins, h...|[stori, begin, ha...|[0.02682894971037...|1,0,0,0,0,0,0,0,0...|\n",
      "| 2462689|       Meet John Doe|Infuriated at bei...|['Black-and-white...|[infuriated, at, ...|[infuriated, told...|[infuri, told, wr...|[0.04249150506208...|1,1,1,0,0,0,0,0,1...|\n",
      "|20532852|Destination Meatball|A line of people ...|['Animation', 'Sh...|[a, line, of, peo...|[line, people, dr...|[line, peopl, dro...|[0.11612121125493...|0,0,0,0,0,0,0,0,0...|\n",
      "|15401493|    Husband for Hire|Lola  attempts to...|          ['Comedy']|[lola, attempts, ...|[lola, attempts, ...|[lola, attempt, g...|[0.02442255383298...|0,1,0,0,0,0,0,0,0...|\n",
      "|18188932|         Up and Down|Milan and Goran a...|['Crime Fiction',...|[milan, and, gora...|[milan, goran, tw...|[milan, goran, tw...|[-0.0226330438036...|1,1,0,0,0,1,1,0,0...|\n",
      "| 2940516|Ghost In The Noon...|Bumbling pirate c...|          ['Comedy']|[bumbling, pirate...|[bumbling, pirate...|[bumbl, pirat, cr...|[0.00641190038175...|0,1,0,0,0,0,0,0,0...|\n",
      "| 1480747|       House Party 2|{{plot}} Followin...|          ['Comedy']|[plot, following,...|[plot, following,...|[plot, follow, su...|[0.05220647962274...|0,1,0,0,0,0,0,0,0...|\n",
      "|24448645|Forest of the Dam...|Despite Lucy's re...|          ['Horror']|[despite, lucy, s...|[despite, lucy, r...|[despit, luci, re...|[0.04021538769502...|0,0,0,0,0,0,0,1,0...|\n",
      "|15072401|Charlie Chan's Se...|Alan Colby, heir ...|['Crime Fiction',...|[alan, colby, hei...|[alan, colby, hei...|[alan, colbi, hei...|[0.01593764940431...|0,0,0,1,0,0,1,1,0...|\n",
      "| 4018288|     The Biggest Fan|Debbie's favorite...|           ['Drama']|[debbie, s, favor...|[debbie, favorite...|[debbi, favorit, ...|[0.04640913030171...|1,0,0,0,0,0,0,0,0...|\n",
      "| 4596602|      Ashes to Ashes|Ashes to Ashes is...|['Crime Fiction',...|[ashes, to, ashes...|[ashes, ashes, se...|[ash, ash, set, l...|[0.01876641378241...|0,0,1,1,1,0,1,0,0...|\n",
      "|15224586|        Green Dragon|The film follows ...|  ['Indie', 'Drama']|[the, film, follo...|[film, follows, e...|[film, follow, ex...|[-0.0068212305570...|1,0,0,0,0,0,0,0,0...|\n",
      "|15585766|  The Rats of Tobruk|Three friends are...|           ['Drama']|[three, friends, ...|[three, friends, ...|[three, friend, d...|[0.00201119128614...|1,0,0,0,0,0,0,0,0...|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vectW2V.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+------+------+------+------+------+------+------+------+------+\n",
      "|            features|                  op|NAME1|NAME2|NAME3|NAME4|NAME5|NAME6|NAME7|NAME8|NAME9|NAME10|NAME11|NAME12|NAME13|NAME14|NAME15|NAME16|NAME17|NAME18|NAME19|NAME20|\n",
      "+--------------------+--------------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+------+------+------+------+------+------+------+------+------+\n",
      "|[0.03008223511963...|1,0,0,0,0,1,0,0,0...|    1|    0|    0|    0|    0|    1|    0|    0|    0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|\n",
      "|[0.04306361994775...|1,0,0,0,1,0,0,0,0...|    1|    0|    0|    0|    1|    0|    0|    0|    0|     0|     1|     0|     0|     0|     0|     0|     0|     1|     0|     0|\n",
      "|[-0.0210946972665...|1,0,0,0,1,0,0,0,0...|    1|    0|    0|    0|    1|    0|    0|    0|    0|     0|     0|     0|     0|     0|     0|     0|     1|     0|     0|     0|\n",
      "|[0.02432931015478...|0,1,0,0,0,0,0,0,0...|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|\n",
      "|[0.01604021873983...|1,0,0,0,0,1,1,0,0...|    1|    0|    0|    0|    0|    1|    1|    0|    0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|\n",
      "|[0.06528372014330...|1,0,0,1,1,0,0,0,0...|    1|    0|    0|    1|    1|    0|    0|    0|    0|     0|     1|     0|     0|     0|     0|     0|     0|     0|     0|     0|\n",
      "|[0.03373993718494...|1,0,0,1,0,0,0,1,0...|    1|    0|    0|    1|    0|    0|    0|    1|    0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|\n",
      "|[0.02682894971037...|1,0,0,0,0,0,0,0,0...|    1|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|\n",
      "|[0.04249150506208...|1,1,1,0,0,0,0,0,1...|    1|    1|    1|    0|    0|    0|    0|    0|    1|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     1|\n",
      "|[0.11612121125493...|0,0,0,0,0,0,0,0,0...|    0|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|     0|     1|     1|     0|     1|     0|     0|     0|     0|\n",
      "|[0.02442255383298...|0,1,0,0,0,0,0,0,0...|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|\n",
      "|[-0.0226330438036...|1,1,0,0,0,1,1,0,0...|    1|    1|    0|    0|    0|    1|    1|    0|    0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|\n",
      "|[0.00641190038175...|0,1,0,0,0,0,0,0,0...|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|\n",
      "|[0.05220647962274...|0,1,0,0,0,0,0,0,0...|    0|    1|    0|    0|    0|    0|    0|    0|    0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|\n",
      "|[0.04021538769502...|0,0,0,0,0,0,0,1,0...|    0|    0|    0|    0|    0|    0|    0|    1|    0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|\n",
      "|[0.01593764940431...|0,0,0,1,0,0,1,1,0...|    0|    0|    0|    1|    0|    0|    1|    1|    0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     1|     0|\n",
      "|[0.04640913030171...|1,0,0,0,0,0,0,0,0...|    1|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|\n",
      "|[0.01876641378241...|0,0,1,1,1,0,1,0,0...|    0|    0|    1|    1|    1|    0|    1|    0|    0|     1|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|\n",
      "|[-0.0068212305570...|1,0,0,0,0,0,0,0,0...|    1|    0|    0|    0|    0|    0|    0|    0|    0|     1|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|\n",
      "|[0.00201119128614...|1,0,0,0,0,0,0,0,0...|    1|    0|    0|    0|    0|    0|    0|    0|    0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|\n",
      "+--------------------+--------------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+------+------+------+------+------+------+------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vectW2V = df_vectW2V.withColumn(\"op\", iplookup_udf(\"genre\"))\n",
    "NewdfW2V = df_vectW2V.select('features','op')\n",
    "split_col = pyspark.sql.functions.split(NewdfW2V['op'], ',')\n",
    "for i in range(20):\n",
    "    NewdfW2V = NewdfW2V.withColumn('NAME'+str(i+1), split_col.getItem(i))\n",
    "\n",
    "NewdfW2V.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'model_rfcW2V'\n",
    "os.mkdir(path)\n",
    "\n",
    "listop = []\n",
    "listl = ['Drama','Comedy','Romance Film','Thriller','Action','World cinema','Crime Fiction','Horror','Black-and-white',\n",
    "         'Indie','Action/Adventure','Adventure','Family Film','Short Film','Romantic drama','Animation','Musical','Science Fiction',\n",
    "        'Mystery','Romantic comedy']\n",
    "\n",
    "opsW2V = pd.DataFrame()\n",
    "opsW2V['movie_id'] = [int(row.movie_id) for row in df_testW2V.collect()]\n",
    "\n",
    "for i in range(len(listl)):\n",
    "    dramadf = NewdfW2V.select('features','NAME'+str(i+1))\n",
    "    dramadf = dramadf.withColumnRenamed('NAME'+str(i+1),'label')\n",
    "    dramadf = dramadf.withColumn(\"label\", dramadf[\"label\"].cast(IntegerType()))\n",
    "    \n",
    "    ################ set pipeline ########################\n",
    "    lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=30,family = \"binomial\")\n",
    "    \n",
    "    modelN = lr.fit(dramadf)\n",
    "    modelN.save(os.path.join(sys.argv[1], 'model_'+str(i+1)))\n",
    "    predictionsW2V = modelN.transform(df_testW2V)\n",
    "    ################ store ouputs offline ##################\n",
    "    mvv_array = [int(row.prediction) for row in predictionsW2V.collect()]\n",
    "    opsW2V['op'+str(i+1)] = mvv_array\n",
    "\n",
    "colsW2V = ['op1','op2','op3','op4','op5','op6','op7','op8','op9','op10',\n",
    "        'op11','op12','op13','op14','op15','op16','op17','op18','op19','op20']\n",
    "\n",
    "opsW2V.head()\n",
    "FinalOutputW2V = pd.DataFrame() \n",
    "FinalOutputW2V['movie_id'] = opsW2V['movie_id']\n",
    "FinalOutputW2V['predictions'] = opsW2V[colsW2V].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "FinalOutputW2V.to_csv('OutputW2VNew.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo run to test accuracy on one genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1DFW2V = NewdfW2V.select('features','NAME2')\n",
    "F1DFW2V = F1DFW2V.withColumnRenamed('NAME2','label')\n",
    "F1DFW2V = F1DFW2V.withColumn(\"label\", F1DFW2V[\"label\"].cast(IntegerType()))\n",
    "trainingW2V, testingW2V = F1DFW2V.randomSplit([0.7,0.3], seed=100)\n",
    "\n",
    "lrW2V = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=25, family = \"binomial\")\n",
    "lrModelW2V = lrW2V.fit(trainingW2V)\n",
    "predictionsW2V = lrModelW2V.transform(testingW2V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       accuracy:0.7339256326311449:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "evaluator_cv_lrTF = MulticlassClassificationEvaluator().setPredictionCol(\"prediction\").evaluate(predictionsW2V)\n",
    "print('                       accuracy:{}:'.format(evaluator_cv_lrTF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To load the saved models and to get predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Change the path to the 'modelsQ1' or 'modelsQ2' or 'modelsQ3' folder as required\n",
    "2. Update the Variable names of DataFrame variable 'DATAIN' to any one of the following as required.\n",
    "                Question 1 : df_vectT\n",
    "                Question 2 : df_testTDF\n",
    "                Question 3 : df_testW2V\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "\n",
    "\n",
    "listl = ['Drama','Comedy','Romance Film','Thriller','Action','World cinema','Crime Fiction','Horror','Black-and-white',\n",
    "         'Indie','Action/Adventure','Adventure','Family Film','Short Film','Romantic drama','Animation','Musical','Science Fiction',\n",
    "        'Mystery','Romantic comedy']\n",
    "\n",
    "### Update the following variable \n",
    "DATAIN = df_testTDF\n",
    "\n",
    "outputPd = pd.DataFrame()\n",
    "outputPd['movie_id'] = [int(row.movie_id) for row in DATAIN.collect()]\n",
    "\n",
    "for i in range(len(listl)):\n",
    "    k = i+1\n",
    "    \n",
    "    #### Update the path as required\n",
    "    LogisticRegressionModel.load('/home/cse587/Project3/modelsQ2/'+'model_'+str(k))\n",
    "    predictionsN = modelN.transform(DATAIN)\n",
    "    mvv_array = [int(row.prediction) for row in predictionsN.collect()]\n",
    "    outputPd['op'+str(i+1)] = mvv_array\n",
    "\n",
    "cols = ['op1','op2','op3','op4','op5','op6','op7','op8','op9','op10',\n",
    "        'op11','op12','op13','op14','op15','op16','op17','op18','op19','op20']\n",
    "\n",
    "\n",
    "FinalOutputTF = pd.DataFrame() \n",
    "FinalOutputTF['movie_id'] = outputPd['movie_id']\n",
    "FinalOutputTF['predictions'] = outputPd[cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "FinalOutputTF.to_csv('QuestionOutput.csv')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
